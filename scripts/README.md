


Workflow
========

1. Get listings .. one LOTUS job per table, output in inputs/
    - run bash ls
    - consolidate into listings by CMOR variables
1. Scan by table:
    - Get a listing of variables for each table
    - Run through each variable, creating a range-check shelve file for each netcdf file.
3. Consolidate
    - Consolidate each time series into a single record in a json file
    - Consolidate across models, to generate a single json file for each experiment/CMOR variable combination.
4. Analysis
    - Plots etc.

File Structure
==============

range check shelve file: contains `__info__`, `__tech__` and one record for each input netcdf file. This file is generated by the `scan_files.ScanFile` class.

The input file record contains:

    - boolean, True;
    - class version;
    - time stamp;
    - tech info (dictionary);
    - flags;
    - summary info (7 floats and one integer);
    - absolute mean by time slice (list of floats);
    - percentiles by time slice (list of list of floats).

The `__tech__` record contains a dictionary. Currently one key: `percentiles` which contains a list of the precentiles (13 at present).




